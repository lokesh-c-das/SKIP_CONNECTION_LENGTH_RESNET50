{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "RESNET_BY_US_WITH_SKIP_CONNECTION_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz-T7ILCuImn"
      },
      "source": [
        "# Useful Links\n",
        "# 1. https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/\n",
        "# 2. https://machinelearningknowledge.ai/keras-implementation-of-resnet-50-architecture-from-scratch/\n",
        "# 3. https://towardsdatascience.com/resnets-for-cifar-10-e63e900524e0 (Understanding the resnet architecture for cifar10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpbYz2fluImp"
      },
      "source": [
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dYc2cT0QGrY",
        "outputId": "2b93b2a4-a67c-4ef4-dd65-d66ae0a42288"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "PATH = '/content/drive/MyDrive/PhD Studies/fall21/ADVML/results/skip3/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaI1M8asmPcd"
      },
      "source": [
        "# RESNET50 WITH SKIP CONNECTION 3: Implemented by Lokesh"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zG6OuColuImq"
      },
      "source": [
        " for i in range(5):\n",
        "  # Load necessary libraries\n",
        "  import os\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  #import cv2\n",
        "  import tensorflow as tf\n",
        "  from tensorflow import keras\n",
        "  from tensorflow.keras import backend as K\n",
        "  from tensorflow.keras.optimizers import SGD, Adam\n",
        "  #from google.colab.patches import cv2_imshow\n",
        "  from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n",
        "  from tensorflow.keras.models import Sequential, Model,load_model\n",
        "  from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "  from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D,MaxPool2D,Dropout\n",
        "  from tensorflow.keras.preprocessing import image\n",
        "  from tensorflow.keras.initializers import glorot_uniform\n",
        "  from matplotlib import pyplot as plt\n",
        "  from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "  def load_dataset(dName=\"CIFAR10\"):\n",
        "      dataset = None\n",
        "      num_classes = None\n",
        "      if dName == \"CIFAR10\":\n",
        "          num_classes = 10\n",
        "          dataset = tf.keras.datasets.cifar10.load_data()\n",
        "      if dName == \"CIFAR100\":\n",
        "          num_classes = 100\n",
        "          dataset = tf.keras.datasets.cifar100.load_data()\n",
        "      (X_train, y_train), (X_test, y_test) = dataset\n",
        "      # Convert target value to categorical values\n",
        "      # One-hot-encoded target values\n",
        "      y_train = to_categorical(y_train,num_classes)\n",
        "      y_test = to_categorical(y_test, num_classes)\n",
        "      \n",
        "      return (X_train, y_train),(X_test, y_test)\n",
        "\n",
        "\n",
        "  def divideDataset(X_train, y_train, X_test,y_test):\n",
        "      dataLength = X_train.shape[0]\n",
        "      trainLen=0\n",
        "      dataTrain = []\n",
        "      dataTest = []\n",
        "      percent = 0.2\n",
        "      while(trainLen<dataLength):\n",
        "          trainLen = int(X_train.shape[0]*percent)\n",
        "          print(X_train[:trainLen].shape)\n",
        "          train = (X_train[:trainLen],y_train[:trainLen])\n",
        "          retriveLen = int(X_test.shape[0]*percent)\n",
        "          test = (X_test[:retriveLen],y_test[:retriveLen])\n",
        "          #print(tuple(train))\n",
        "          dataTrain.append(train)\n",
        "          dataTest.append(test)\n",
        "          print(\"\\n\")\n",
        "          percent +=0.2\n",
        "      return dataTrain, dataTest\n",
        "\n",
        "  (X_train, y_train),(X_test, y_test) = load_dataset()\n",
        "  dataTrain, dataTest = divideDataset(X_train, y_train, X_test, y_test) # this contains the list of 5 different datasets\n",
        "  def describeDataset(X_train, y_train, X_test, y_test):\n",
        "      print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
        "      print('Test: X=%s, y=%s' % (X_test.shape, y_test.shape))\n",
        "\n",
        "  def normalizeInput(X_train,X_test):\n",
        "      X_train = X_train.astype('float32')\n",
        "      X_test = X_test.astype('float32')\n",
        "      mean = np.mean(X_train, axis=(0,1,2,3))\n",
        "      std = np.std(X_train,axis=(0,1,2,3))\n",
        "      X_train = (X_train-mean)/(std+1e-7)\n",
        "      X_test = (X_test-mean)/(std+1e-7)\n",
        "      \n",
        "      return X_train, X_test\n",
        "\n",
        "  # Resnet50 \n",
        "  def identityBlock(X, f, filters, stage, block, p=0.0):\n",
        "      k_initializer = glorot_uniform(seed=0)\n",
        "      conv_name = 'res'+str(stage)+block+'_branch'\n",
        "      bat_name = 'bn'+str(stage)+block+'_branch'\n",
        "      \n",
        "      F1, F2, F3 = filters\n",
        "      \n",
        "      X_skip = X\n",
        "      \n",
        "      X = Conv2D(filters=F1,kernel_size=(1,1),strides =(1,1),padding='valid',name=conv_name+'2a',\n",
        "                kernel_initializer=k_initializer)(X)\n",
        "      X = BatchNormalization(axis=3,name=bat_name+'2a')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = Dropout(p)(X)\n",
        "      \n",
        "      X = Conv2D(filters=F2, kernel_size=(f,f),strides=(1,1),padding='same',name=conv_name+'2b',\n",
        "                kernel_initializer=k_initializer)(X)\n",
        "      X = BatchNormalization(axis=3,name=bat_name+'2b')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = Dropout(p)(X)\n",
        "      \n",
        "      X = Conv2D(filters=F3,kernel_size=(1,1),strides=(1,1),padding='valid',name=conv_name+'2c',\n",
        "                kernel_initializer=k_initializer)(X)\n",
        "      X = BatchNormalization(axis=3,name=bat_name+'2c')(X)\n",
        "      \n",
        "      X = Add()([X,X_skip])\n",
        "      X = Activation('relu')(X)\n",
        "      X = Dropout(p)(X)\n",
        "      \n",
        "      return X\n",
        "\n",
        "  def convolutionalBlock(X, f, filters, stage, block, s=2,p=0.0):\n",
        "      k_init = glorot_uniform(seed=0)\n",
        "      conv_name = 'res'+str(stage)+block+'_branch'\n",
        "      bat_name = 'bn'+str(stage)+block+'_branch'\n",
        "      \n",
        "      F1, F2, F3 = filters\n",
        "      \n",
        "      X_skip = X\n",
        "      \n",
        "      X = Conv2D(filters=F1,kernel_size=(1,1),strides=(s,s),name=conv_name+'2a',kernel_initializer=k_init)(X)\n",
        "      X = BatchNormalization(axis=3,name=bat_name+'2a')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = Dropout(p)(X)\n",
        "      \n",
        "      X = Conv2D(filters=F2, kernel_size=(f,f),strides=(1,1),padding='same', name=conv_name+'2b',kernel_initializer=k_init)(X)\n",
        "      X = BatchNormalization(axis=3,name=bat_name+'2b')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = Dropout(p)(X)\n",
        "      \n",
        "      X = Conv2D(filters=F3, kernel_size=(1,1),strides=(1,1),padding='valid',name=conv_name+'2c',kernel_initializer=k_init)(X)\n",
        "      X = BatchNormalization(axis=3,name=bat_name+'2c')(X)\n",
        "      \n",
        "      X_skip = Conv2D(filters=F3,kernel_size=(1,1),strides=(s,s), padding='valid',name=conv_name+'1',\n",
        "                    kernel_initializer=k_init)(X_skip)\n",
        "      X_skip = BatchNormalization(axis=3,name=bat_name+'1')(X_skip)\n",
        "      \n",
        "      X = Add()([X, X_skip])\n",
        "      X = Activation('relu')(X)\n",
        "      X = Dropout(p)(X)\n",
        "      \n",
        "      return X\n",
        "  def ResNet50(input_shape=(64,64,3), classes=10):\n",
        "      dropoutProb = 0.0\n",
        "      X_input = Input(shape=(input_shape))\n",
        "      X = ZeroPadding2D((3,3))(X_input)\n",
        "      \n",
        "      \n",
        "      X = Conv2D(64,kernel_size=(7,7),strides=(2,2),name='conv1',kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "      X = BatchNormalization(axis=3,name='bn_conv1')(X)\n",
        "      X = Activation('relu')(X)\n",
        "      X = MaxPooling2D((3,3),strides=(2,2))(X)\n",
        "      \n",
        "      X = convolutionalBlock(X, f=3, filters=[64,64,256],stage=2,block='a',s=1)\n",
        "      X = identityBlock(X,3,[64,64,256],stage=2,block='b')\n",
        "      X = identityBlock(X,3,[64,64,256],stage=2,block='c')\n",
        "      \n",
        "      \n",
        "      X = convolutionalBlock(X, f=3, filters=[128,128,512],stage=3,block='a',s=2)\n",
        "      X = identityBlock(X,3,[128,128,512],stage=3,block='b')\n",
        "      X = identityBlock(X,3,[128,128,512],stage=3,block='c')\n",
        "      X = identityBlock(X,3,[128,128,512],stage=3,block='d')\n",
        "    \n",
        "      \n",
        "      X = convolutionalBlock(X, f=3, filters=[256,256,1024],stage=4,block='a',s=2)\n",
        "      X = identityBlock(X,3,[256,256,1024],stage=4,block='b')\n",
        "      X = identityBlock(X,3,[256,256,1024],stage=4,block='c')\n",
        "      X = identityBlock(X,3,[256,256,1024],stage=4,block='d')\n",
        "      X = identityBlock(X,3,[256,256,1024],stage=4,block='e')\n",
        "      X = identityBlock(X,3,[256,256,1024],stage=4,block='f')\n",
        "      \n",
        "      \n",
        "      \n",
        "      X = convolutionalBlock(X, f=3, filters=[512,512,2048],stage=5,block='a',s=2)\n",
        "      X = identityBlock(X,3,[512,512,2048],stage=5,block='b')\n",
        "      X = identityBlock(X,3,[512,512,2048],stage=5,block='c')\n",
        "      \n",
        "      \n",
        "      X = AveragePooling2D((2,2),name='avg_pool',padding='same')(X)\n",
        "      \n",
        "      X = Flatten()(X)\n",
        "      X = Dropout(dropoutProb)(X)\n",
        "      X = Dense(classes, activation='softmax',name='fc'+str(classes),kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "      \n",
        "      model = Model(inputs=X_input,outputs=X, name='LResNet50')\n",
        "      \n",
        "      return model\n",
        "  model = ResNet50(input_shape=(32,32,3),classes=10)\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "  X_train, X_test = normalizeInput(dataTrain[i][0], dataTest[i][0])\n",
        "  y_train = dataTrain[i][1]\n",
        "  y_test = dataTest[i][1]\n",
        "  earlyStop = EarlyStopping(monitor='loss',patience=5,mode='auto')\n",
        "  csvLogger = CSVLogger(\"/content/drive/MyDrive/PhD Studies/fall21/ADVML/results/skip3/\"+str(i)+\".csv\")\n",
        "  history = model.fit(X_train,y_train, epochs=300,batch_size=256,validation_data=(X_test,y_test),verbose=1, callbacks=[csvLogger,earlyStop])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}